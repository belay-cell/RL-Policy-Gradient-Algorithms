# RL Policy Gradient Algorithms

**Implementation of REINFORCE, A2C, and DDPG** using PyTorch and Gymnasium for classic control tasks.

> Professional refactoring of KAIST CS377 Reinforcement Learning coursework into production-ready code.

## ğŸ¯ Overview

This repository contains modular, well-documented implementations of three fundamental policy gradient algorithms:

- **REINFORCE** (Vanilla + Baseline): Monte Carlo policy gradient
- **A2C** (Advantage Actor-Critic): Synchronous actor-critic with online learning  
- **DDPG** (Deep Deterministic Policy Gradient): Off-policy for continuous control

### Environments

| Environment | Action Space | Obs Space | Algorithms |
|------------|--------------|-----------|------------|
| **CartPole-v1** | Discrete (2) | Continuous (4D) | REINFORCE, A2C |
| **Pendulum-v1** | Continuous [-2, 2] | Continuous (3D) | DDPG |

## ğŸ“ˆ Results (From Actual Training)

### Training Performance

| Algorithm | Environment | Episodes to Solve | Final Avg Reward |
|-----------|-------------|-------------------|------------------|
| **REINFORCE (Vanilla)** | CartPole-v1 | **262** | **450.80** |
| **REINFORCE (Baseline)** | CartPole-v1 | **495** | **461.60** |
| **A2C** | CartPole-v1 | **699** | **453.50** |
| **DDPG** | Pendulum-v1 | **172** | **-195.39** |

### Key Insights

âœ… **REINFORCE vanilla solved fastest** (262 episodes) - simple but effective  
âœ… **Baseline version** reduced variance significantly  
âœ… **A2C** enables online learning without episode waits  
âœ… **DDPG** efficiently handles continuous actions with replay buffer

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/belay-cell/RL-Policy-Gradient-Algorithms.git
cd RL-Policy-Gradient-Algorithms
pip install -r requirements.txt
```

## ğŸš€ Usage

### Train REINFORCE on CartPole

```python
import gymnasium as gym
from models import SoftmaxPolicy
from reinforce import REINFORCE

env = gym.make("CartPole-v1")
policy = SoftmaxPolicy(env.observation_space.shape[0], env.action_space.n)
agent = REINFORCE(policy, lr=1e-3, solve_criteria=450, episode_limit=1000)
total_rewards = agent.train(env)
```

### Train DDPG on Pendulum

```python
import gymnasium as gym
from models import DDPGActor, DDPGCritic
from utils import ReplayBuffer
from ddpg import DDPG

env = gym.make("Pendulum-v1")
actor = DDPGActor(3, 1, action_range=2.0)
critic = DDPGCritic(3, 1)
buffer = ReplayBuffer(10000)
agent = DDPG(buffer, actor, critic, actor_lr=1e-4, critic_lr=1e-3)
total_rewards = agent.train(env)
```

## ğŸ“š Project Structure

```
RL-Policy-Gradient-Algorithms/
â”œâ”€â”€ models.py          # Neural network architectures
â”œâ”€â”€ reinforce.py       # REINFORCE (vanilla + baseline)
â”œâ”€â”€ a2c.py             # Advantage Actor-Critic  
â”œâ”€â”€ ddpg.py            # Deep Deterministic Policy Gradient
â”œâ”€â”€ utils.py           # Replay buffer utilities
â”œâ”€â”€ requirements.txt   # Dependencies
â”œâ”€â”€ README.md         # This file
â””â”€â”€ LICENSE           # MIT License
```

## ğŸ—ï¸ Architecture

### Neural Networks

**SoftmaxPolicy** (Discrete Actions - CartPole)
```
state (4D) â†’ FC(128) â†’ ReLU â†’ FC(64) â†’ ReLU â†’ FC(2) â†’ Softmax â†’ action_probs
```

**Critic** (Value Function - A2C)
```
state â†’ FC(128) â†’ ReLU â†’ FC(64) â†’ ReLU â†’ FC(1) â†’ value
```

**DDPGActor** (Continuous Actions - Pendulum)
```
state (3D) â†’ FC(128) â†’ ReLU â†’ FC(64) â†’ ReLU â†’ FC(1) â†’ Tanh â†’ action * 2.0
```

**DDPGCritic** (State-Action Value)
```
concat(state, action) â†’ FC(128) â†’ ReLU â†’ FC(64) â†’ ReLU â†’ FC(1) â†’ Q-value
```

## ğŸ”¬ Algorithm Details

### REINFORCE

**Policy Gradient Theorem:**
```
âˆ‡Î¸ J(Î¸) = ğ”¼[âˆ‡Î¸ log Ï€Î¸(a|s) Â· G_t]
```

**With Baseline:**
```
Î¸ â† Î¸ + Î± âˆ‘_t (G_t - baseline) âˆ‡Î¸ log Ï€Î¸(a_t|s_t)
```

### A2C

**Advantage Function:**
```
A(s_t, a_t) = R_{t+1} + Î³V(s_{t+1}) - V(s_t)
```

**Update Rules:**
```
Actor:  Î¸Ï€ â† Î¸Ï€ + Î± âˆ‡Î¸Ï€[A Â· log Ï€(a|s)]
Critic: Î¸v â† Î¸v - Î± âˆ‡Î¸v[V(s) - (R + Î³V(s'))]^2
```

### DDPG

**Key Features:**
- Deterministic policy: `a = Î¼(s)`
- Experience replay (capacity: 10,000)
- Target networks with soft updates (Ï„=0.005)
- Gaussian exploration noise

## ğŸ“Š Hyperparameters

| Parameter | REINFORCE | A2C | DDPG |
|-----------|-----------|-----|------|
| Learning Rate | 1e-3 | Actor:1e-3, Critic:1e-3 | Actor:1e-4, Critic:1e-3 |
| Discount (Î³) | 0.99 | 0.99 | 0.99 |
| Batch Size | Full episode | 1 (online) | 32 |
| Replay Buffer | - | - | 10,000 |
| Soft Update (Ï„) | - | - | 0.005 |
| Solve Criteria | 450 | 450 | -200 |

## ğŸ§ª Technical Highlights

- âœ… **Type Hints**: Full Python type annotations
- âœ… **Modular Design**: Separate files for each algorithm
- âœ… **Reproducible**: Fixed seed (54321) for consistent results
- âœ… **Efficient**: Vectorized PyTorch operations
- âœ… **Clean Code**: PEP 8 compliant
- âœ… **Progress Tracking**: tqdm progress bars with live stats

## ğŸ“ Key Learnings

1. **Variance Matters**: Baseline in REINFORCE dramatically stabilizes training
2. **Online vs Batch**: A2C updates every step vs REINFORCE waits for full episodes
3. **Exploration**: DDPG needs Gaussian noise since policy is deterministic
4. **Stability**: Target networks in DDPG prevent Q-value divergence

## ğŸ”— References

- Sutton & Barto. *Reinforcement Learning: An Introduction* (2nd ed.), Chapter 13
- Mnih et al. *Asynchronous Methods for Deep Reinforcement Learning* (2016)
- Lillicrap et al. *Continuous Control with Deep Reinforcement Learning* (2015)

## ğŸ’¼ Portfolio Showcase

**This project demonstrates:**

- âœ… Deep RL algorithm implementation from scratch
- âœ… PyTorch proficiency for neural network development  
- âœ… Clean code architecture and software engineering best practices
- âœ… Technical documentation and reproducible research
- âœ… MLOps readiness for production deployment

> *Originally developed for KAIST CS377 Reinforcement Learning (Student ID: 20220934), refactored into professional-grade codebase.*

## ğŸ›¡ï¸ License

MIT License - See [LICENSE](LICENSE)

## ğŸš€ Author

**Belay Zeleke**  
GitHub: [@belay-cell](https://github.com/belay-cell)  
Interested in MLOps, Reinforcement Learning, and Production ML Systems

---

**Built with PyTorch ğŸ”¥ | Trained on Gymnasium Environments ğŸ®**
